
@article{khanam_comparative_2025,
	title = {Comparative {Performance} {Evaluation} of {YOLOv5}, {YOLOv8}, and {YOLOv11} for {Solar} {Panel} {Defect} {Detection}},
	volume = {5},
	issn = {2673-9941},
	url = {https://www.mdpi.com/2673-9941/5/1/6},
	doi = {10.3390/solar5010006},
	abstract = {The reliable operation of photovoltaic (PV) systems is essential for sustainable energy production, yet their efficiency is often compromised by defects such as bird droppings, cracks, and dust accumulation. Automated defect detection is critical for addressing these challenges in large-scale solar farms, where manual inspections are impractical. This study evaluates three YOLO object detection models—YOLOv5, YOLOv8, and YOLOv11—on a comprehensive dataset to identify solar panel defects. YOLOv5 achieved the fastest inference time (7.1 ms per image) and high precision (94.1\%) for cracked panels. YOLOv8 excelled in recall for rare defects, such as bird drops (79.2\%), while YOLOv11 delivered the highest mAP@0.5 (93.4\%), demonstrating a balanced performance across the defect categories. Despite the strong performance for common defects like dusty panels (mAP@0.5 {\textgreater} 98\%), bird drop detection posed challenges due to dataset imbalances. These results highlight the trade-offs between accuracy and computational efficiency, providing actionable insights for deploying automated defect detection systems to enhance PV system reliability and scalability.},
	number = {1},
	journal = {Solar},
	author = {Khanam, Rahima and Asghar, Tahreem and Hussain, Muhammad},
	year = {2025},
}

@article{pecolt_personal_2025,
	title = {Personal {Identification} {Using} {Embedded} {Raspberry} {Pi}-{Based} {Face} {Recognition} {Systems}},
	volume = {15},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/15/2/887},
	doi = {10.3390/app15020887},
	abstract = {Facial recognition technology has significantly advanced in recent years, with promising applications in fields ranging from security to consumer electronics. Its importance extends beyond convenience, offering enhanced security measures for sensitive areas and seamless user experiences in everyday devices. This study focuses on the development and validation of a facial recognition system utilizing a Haar cascade classifier and the AdaBoost machine learning algorithm. The system leverages characteristic facial features—distinct, measurable attributes used to identify and differentiate faces within images. A biometric facial recognition system was implemented on a Raspberry Pi microcomputer, capable of detecting and identifying faces using a self-contained reference image database. Verification involved selecting the similarity threshold, a critical factor influencing the balance between accuracy, security, and user experience in biometric systems. Testing under various environmental conditions, facial expressions, and user demographics confirmed the system’s accuracy and efficiency, achieving an average recognition time of 10.5 s under different lighting conditions, such as daylight, artificial light, and low-light scenarios. It is shown that the system’s accuracy and scalability can be enhanced through testing with larger databases, hardware upgrades like higher-resolution cameras, and advanced deep learning algorithms to address challenges such as extreme facial angles. Threshold optimization tests with six male participants revealed a value that effectively balances accuracy and efficiency. While the system performed effectively under controlled conditions, challenges such as biometric similarities and vulnerabilities to spoofing with printed photos underscore the need for additional security measures, such as thermal imaging. Potential applications include access control, surveillance, and statistical data collection, highlighting the system’s versatility and relevance.},
	number = {2},
	journal = {Applied Sciences},
	author = {Pecolt, Sebastian and Błażejewski, Andrzej and Królikowski, Tomasz and Maciejewski, Igor and Gierula, Kacper and Glowinski, Sebastian},
	year = {2025},
}

@article{bai_decade_2025,
	title = {A {Decade} of {Video} {Analytics} at {Edge}: {Training}, {Deployment}, {Orchestration}, and {Platforms}},
	volume = {28},
	doi = {10.1109/COMST.2025.3563377},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Bai, Tong and Zhao, Haoran and Huang, Lei and Wang, Zhipeng and In Kim, Dong and Nallanathan, Arumugam},
	year = {2025},
	keywords = {Edge computing, Security, deep learning, computer vision, Artificial intelligence, Streaming media, Computational modeling, Visual analytics, Surveillance, Surveys, Training, Reviews, Tutorials, edge intelligence, video analytics},
	pages = {2127--2162},
}

@inproceedings{chen_florence-vl_2025,
	title = {Florence-{VL}: {Enhancing} {Vision}-{Language} {Models} with {Generative} {Vision} {Encoder} and {Depth}-{Breadth} {Fusion}},
	doi = {10.1109/CVPR52734.2025.02321},
	booktitle = {2025 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Chen, Jiuhai and Yang, Jianwei and Wu, Haiping and Li, Dianqi and Gao, Jianfeng and Zhou, Tianyi and Xiao, Bin},
	year = {2025},
	keywords = {Computer vision, Computational modeling, Feature extraction, Transformers, Visualization, Training, Benchmark testing, Tuning, Optical character recognition, Statistical analysis},
	pages = {24928--24938},
}

@article{xia_enhancing_2025,
	title = {Enhancing {Visual} {Analysis} in {Person} {Reidentification} {With} {Vision}–{Language} {Models}},
	volume = {45},
	doi = {10.1109/MCG.2025.3593227},
	number = {6},
	journal = {IEEE Computer Graphics and Applications},
	author = {Xia, Wang and Wang, Tianci and Li, Jiawei and Sun, Guodao and Gao, Haidong and Tan, Xu and Liang, Ronghua},
	year = {2025},
	keywords = {Semantics, Machine learning, Visual analytics, Cameras, Public security, Visualization, Data visualization, Clothing, Identification of persons, Image color analysis, Image retrieval, Large language models},
	pages = {44--60},
}

@inproceedings{pham_deep_2025,
	title = {Deep {Learning} and {Graph}-{Based} {Indexing} for {Context}-{Aware} {Video} {Retrieval} {Systems}},
	doi = {10.23919/ICACT63878.2025.10936725},
	booktitle = {2025 27th {International} {Conference} on {Advanced} {Communications} {Technology} ({ICACT})},
	author = {Pham, Dinh-Lam and Le, Lanh Thanh and Chang, Jeong-Hyun and Ahn, Sang-Eun and Sun, Kyonghee and Kim, Kyong-Sook and Kim, Kwanghoon Pio},
	year = {2025},
	keywords = {Deep learning, Data privacy, Monitoring, Communications technology, Proposals, Cameras, Hardware, Costs, Hands, CCTV management system, Contextualization dataset, Deep learning-based system, Graph-based indexing, Indexing, Video contextualization, Video retrieval system},
	pages = {318--322},
}

@inproceedings{lu_real-time_2025,
	title = {Real-{Time} {Analysis} of {Dangerous} {Actions} in {Video} {Surveillance} {Systems} {Based} on {Cloud} {Computing}},
	doi = {10.1109/NNICE64954.2025.11064503},
	booktitle = {2025 5th {International} {Conference} on {Neural} {Networks}, {Information} and {Communication} {Engineering} ({NNICE})},
	author = {Lu, Mingzhao and Xu, Yonggang},
	year = {2025},
	keywords = {Cloud Computing, Edge Computing, Cloud computing, Video surveillance, Deep Learning, Computer architecture, Real-time systems, Streaming media, Transformers, Three-dimensional displays, Attention mechanisms, Usability, Dangerous Action Recognition, Streams, Video Surveillance},
	pages = {664--667},
}

@inproceedings{j_advanced_2025,
	title = {Advanced {Object} {Detection} {Using} {Semantic} and {Temporal} {Query}-{Based} {Frame} {Retrieval} in {Surveillance} {Videos}},
	doi = {10.1109/ICICI65870.2025.11069872},
	abstract = {With the fast expansion of surveillance infrastructure, there is an increasing demand for intelligent systems capable of comprehending and extracting useful information from massive amounts of video footage. This research introduces a novel surveillance architecture that integrates real-time object detection, semantic scene interpretation, and natural language querying to allow for efficient frame-level analysis. The system detects objects with high accuracy using the YOLOv8 model and then extracts spatial relationships to contextualize item interactions inside frames. Our approach includes user-defined parameters that enable filtering based on object type, description, and time range. Beyond detection, our system dynamically generates spatial and proximity relationships between identified items, as well as saves metadata for efficient video frame querying and retrieval. By integrating generative AI, we generate natural language summaries to enhance interpretability from the relationships generated. A Retrieval-Augmented Generation (RAG) model is employed in the final step to match generated summaries with user-provided descriptions. By this method, summaries that best describe user-defined descriptions are analysed, enabling precise retrieval of frame that best match the query. Our implementation demonstrates significant improvements in surveillance analytics by enhancing detection accuracy, retrieval speed, and interpretability of video content.},
	booktitle = {2025 3rd {International} {Conference} on {Inventive} {Computing} and {Informatics} ({ICICI})},
	author = {J, Jeba Sonia and S, Priyadarsshini},
	month = jun,
	year = {2025},
	keywords = {Semantics, Video surveillance, Object detection, Real-time systems, Metadata, Surveillance, Accuracy, Object Detection, Usability, Forensics, LLM based summarization and text-text generation, Metadata storage, Object relationship extraction, RetrievalAugmented Generation (RAG), Semantic scene descriptions, Unsupervised learning, YOLOv8},
	pages = {166--172},
}

@misc{jocher_ultralytics_2024,
	title = {Ultralytics {YOLO11}},
	url = {https://docs.ultralytics.com/models/yolo11/},
	author = {Jocher, Glenn and Qiu, Jing},
	year = {2024},
	annote = {Accessed: 2026-01-04},
}

@inproceedings{bang_reasoning-augmented_2025,
	title = {Reasoning-augmented {Vision}-{Language} {Models} for {Improved} {Competency} in {Scene} {Text} {Recognition}},
	doi = {10.1109/ICCE-Asia67487.2025.11263773},
	booktitle = {2025 {IEEE}/{IEIE} {International} {Conference} on {Consumer} {Electronics}-{Asia} ({ICCE}-{Asia})},
	author = {Bang, Minjae and Han, Juhyeon and Bae, Yuseok and Kim, Sunok and Yun, Kimin},
	year = {2025},
	keywords = {Surveillance, Data mining, Context modeling, Accuracy, Training, Cognition, Training data, Banner understanding, Chain-of-thought, Character recognition, Feeds, Korean Scene Text Recognition, Text recognition, Vision-Language Model (VLM)},
	pages = {1--6},
}

@article{sultana_iot-guard_2019,
	title = {{IoT}-{Guard}: {Event}-{Driven} {Fog}-{Based} {Video} {Surveillance} {System} for {Real}-{Time} {Security} {Management}},
	volume = {7},
	doi = {10.1109/ACCESS.2019.2941978},
	journal = {IEEE Access},
	author = {Sultana, Tanin and Wahid, Khan A.},
	year = {2019},
	keywords = {Internet of Things, IoT, edge, fog, Cloud computing, Video surveillance, Real-time systems, Artificial intelligence, video surveillance, Image edge detection, convolutional neural network, gun-knife detection, message queuing telemetry transport (MQTT), motion detection, real-time security},
	pages = {134881--134894},
}
