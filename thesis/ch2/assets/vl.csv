REF,Fase_FUSE,Argumento_Extraido,Citação_ou_Dado,Como_usar
Bai2025,Contra o Edge / Fase 1,"A implementação de modelos de visão computacional na Edge exige recursos de memória proibitivos para hardware legado (mais de 1GB apenas para modelos leves), obrigando a técnicas de compressão complexas que degradam a precisão ou exigem hardware dedicado caro.","More than 1GB of memory usage is required for running a lightweight MobileNet at approximately 15 fps for 512x512 pixel images - not even running a full CNN model.\" (Sec. II.E)",Usar este dado estatístico na introdução do capítulo para provar matematicamente que correr SAM2 ou Qwen (que são muito mais pesados que o MobileNet) na Edge é impossível sem hardware especializado. Usar também a secção de "Temporal Pruning" para validar a decisão de usar detecção de movimento simples como filtro pré-servidor."
Jeba2025,Fase 3 (Forense e RAG),A simples deteção de objetos é insuficiente para a investigação moderna; o estado da arte exige a integração de LLMs e arquiteturas RAG (Retrieval-Augmented Generation) para transformar deteções em metadados pesquisáveis via linguagem natural - validando a necessidade de processamento pesado centralizado.,"Traditional systems require users to manually browse through hours of footage... Retrieval-Augmented Generation (RAG) model is employed... to match generated summaries with user-provided descriptions." (Sec. I e Abstract),Usar para validar a arquitetura da Fase 3 do FUSE. O artigo prova que a pesquisa forense ("encontra o autocarro amarelo") depende de Bases de Dados Vetoriais e LLMs -  o que justifica a centralização no servidor devido à complexidade destes componentes (Vector DB + LLM).
Khanam2025,Fase 2 (Deteção de Objetos),"O YOLOv11 apresenta-se como a evolução ideal para processamento centralizado, superando o YOLOv8 significativamente em velocidade de inferência (quase o dobro da rapidez) e mantendo a maior precisão (mAP) entre as versões testadas, o que é crítico para a escalabilidade do servidor FUSE.","YOLOv11 offered the most balanced performance, achieving the highest overall mAP@0.5 (93.4%)... inference time of 7.7 ms per image provided a favorable trade-off between accuracy and processing speed [vs YOLOv8 at 15.9 ms].(Sec. 5.3 e 7)","Usar para justificar a escolha técnica do YOLOv11 em detrimento do v8 ou v9. Embora o artigo seja sobre painéis solares, a métrica de performance computacional da arquitetura (velocidade vs precisão) é transponível e valida que o v11 é o mais eficiente para evitar gargalos no servidor."
Pham2025,Armazenamento e Pesquisa (Pós-Fase 3),"A simples acumulação de vídeo é inútil sem mecanismos avançados de indexação; a literatura recente propõe o uso de Grafos e Deep Learning para criar relações contextuais entre objetos e frames, validando a necessidade de uma camada de inteligência centralizada para gestão de grandes volumes de dados.","This study explores the applications of deep learning and graphs to create indexed data... provides an intuitive view for the CCTV management centers in managing and searching content." (Abstract e Sec. I),"Usar na secção final do Estado da Arte para discutir o armazenamento. Serve para argumentar que a inteligência do FUSE não acaba na deteção, mas continua na forma como os dados são organizados (Grafos/Vetores) para permitir a pesquisa intuitiva pelo operador."
Xia2025,Fase 3 (Forense) e Pesquisa,"A literatura de finais de 2025 demonstra que a Re-Identificação de pessoas (Re-ID) eficaz exige o uso de Vision-Language Models (VLMs) para gerar descrições semânticas detalhadas (atributos finos como vestuário e acessórios), superando as limitações dos descritores visuais clássicos e permitindo a pesquisa multimodal.","Existing methods tend to generate descriptions that lack detail... To address these issues... we adopt a vision-language model... to generate detailed textual descriptions and keywords... [creating] a joint search space combining visual and textual information." (Abstract),"Usar para justificar a inclusão do Qwen2-VL na pipeline. O artigo prova que a 'camada semântica' (gerada por VLMs) é necessária para resolver problemas de oclusão e variação de perspetiva que afetam a deteção simples, validando a pesquisa por linguagem natural proposta no FUSE."
Chen2025,Fase 3 (Interpretação Forense e OCR),"A literatura de 2025 demonstra que VLMs baseados em CLIP são insuficientes para tarefas forenses que exigem OCR e localização precisa (Grounding). A solução de ponta exige 'Generative Vision Encoders' com fusão de características em múltiplas profundidades, uma arquitetura computacionalmente intensiva que valida a centralização no servidor.","CLIP and SigLIP... often overlook pixel or region-level details and low-level features... Florence-VL achieves significant improvements... covering general VQA.. perception.. hallucination ... OCR." (Sec. 1),"Usar na Fase 3 para elevar a fasquia da Interpretação Forense. Argumentar que a extração de texto (OCR) e contexto denso exige modelos com arquiteturas de fusão complexas (como Florence-VL ou Qwen2-VL), impossíveis de executar em hardware de borda legado."
Pecolt2025,Contra o Edge (Limitações de Hardware),"Estudos recentes (2025) comprovam que o processamento biométrico em dispositivos de borda (Raspberry Pi), mesmo utilizando algoritmos leves e datados (Haar/AdaBoost), resulta em latências inaceitáveis (~10.5s) e vulnerabilidades de segurança, tornando inviável a execução de modelos modernos (Deep Learning) na ponta.","Achieving an average recognition time of 10.5 s... The system is vulnerable to spoofing... struggles with the real-time processing of large databases." (Abstract e Sec. 1),"Usar na secção de Problematização/Limitações do Edge. Contrastar este dado (10.5s de latência para uma tarefa simples) com a necessidade de tempo real do FUSE, provando que mover a análise para o Servidor é a única forma de usar IA avançada."
Lu2025,Arquitetura Geral e Fase 3,"A centralização do processamento de vídeo em arquitetura Cloud/Servidor permite o uso de modelos baseados em Transformers (ViT) para análise complexa, resultando numa redução de 38% no custo total de propriedade (TCO) e um aumento de 2.5x na capacidade de processamento face a sistemas tradicionais.","Experimental results indicate that this system significantly outperforms traditional systems... reducing total cost of ownership by 38%... processing capacity increases 2.5 times." (Abstract e Sec. V-D),"Usar na secção de Arquitetura para defender o modelo de negócio do FUSE: mover a inteligência para o servidor não é apenas tecnicamente superior (permite Transformers pesados), é economicamente mais viável do que manter hardware de ponta caro e disperso."
Bang2025,Fase 3 (Interpretação Forense e OCR),"A utilização de modelos Qwen2.5-VL com mecanismos de 'Reasoning' (raciocínio passo-a-passo) permite não só ler texto (OCR) em cenários de vigilância, mas também classificar o contexto semântico (ex: tipo de evento), algo impossível com métodos tradicionais e que exige poder computacional centralizado.","We trained the Qwen2.5-VL-3B model as our backbone... incorporating explicit reasoning substantially improves both recognition and classification performance... moving beyond character-level recognition toward contextual understanding." (Abstract e Sec. VI),"Usar para validar a escolha específica da família Qwen-VL na Fase 3. O artigo prova que estes modelos são superiores a métodos antigos para extrair 'significado' de imagens de videovigilância, justificando a necessidade de uma pipeline centralizada que suporte estes modelos pesados."
Sultana2019,Fase 1 (Filtragem de Movimento/Event-Driven),"A implementação de uma arquitetura 'Event-Driven' com filtragem de movimento leve na borda (Edge) reduz o consumo de CPU em cerca de 60% e poupa mais de 99% em largura de banda e armazenamento em comparação com a transmissão contínua para análise, validando a necessidade da Fase 1 do FUSE para viabilizar a análise pesada a jusante.","The proposed IoT-guard provides around 60 percent savings in CPU usage... storage requirement is reduced by 99 percent... substantially BW efficient... due to the minimal amount of data transmission." (Sec. V e Figs. 9-11),"Usar na secção da Fase 1 para justificar a eficiência económica e técnica. Este artigo fornece os números exatos (99% poupança de rede/storage) que provam que o FUSE só é viável se tiver este filtro inicial, libertando o servidor para se focar apenas nos frames relevantes."
Ultralytics2024,Fase 2 (Deteção Técnica),"Relatórios técnicos oficiais (State of the Art 2024) demonstram que o YOLOv11 atinge um novo patamar de eficiência, oferecendo maior precisão (mAP) com 22% menos parâmetros que o YOLOv8m, validando-o como a escolha ideal para maximizar o throughput do servidor FUSE.","YOLO11m achieves a higher mean Average Precision (mAP)... while using 22% fewer parameters than YOLOv8m... ensuring greater computational efficiency." (Ultralytics YOLO11 Docs - Performance Metrics),Usar em conjunto com Khanam2025 para provar que a escolha do v11 não é gosto pessoal.. mas baseada em métricas objetivas de eficiência industrial (menos parâmetros = mais fps no servidor).